{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61613108-fe69-44d6-b323-ed3ba6f4e026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Loaded, proceed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model          import LogisticRegression\n",
    "from sklearn.svm                   import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection       import train_test_split, cross_val_score, LeaveOneOut\n",
    "from sklearn.datasets              import make_classification\n",
    "from sklearn.preprocessing         import StandardScaler\n",
    "from sklearn.metrics               import accuracy_score\n",
    "from sklearn.base                  import clone\n",
    "print('Packages Loaded, proceed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b64e8c-fe7a-40c9-8469-0be460f164e7",
   "metadata": {},
   "source": [
    "I am having this issue where my testing accuracy is consistently higher than my training accuracy, even when my testing accuracy should not be high at all.  This jupyter notebook documents my process.  Not only should the testing accuracy be lower than the validation accuracy, but the testing accuracy should be significantly lower since I would expect the data with many features (2000 x 20) to fit a model that captures much of the noise amongst the redundant variables, even after the feature selection.\n",
    "\n",
    "\n",
    "\n",
    "I am using sklearn in my other models in my projects, so it is likely I have a mistake that I have made elsewhere as well.  My first thought was that perhaps my model was accidentally overwriting the estimator/model and it was causing an artificially inflated accuracy because I was getting a different model each time for the test set (or likewise a deflated accuracy for the validation on the training set)\n",
    "\n",
    "I have this problem relatively frequently and with several datasets, and it there are not a) separate distributions between training/testing set and b) a sample size issue.  I have tried varying the train/test split ratio, doing cross validation manually (in case there was a bug with cross_val_score), implementing different Machine Learning Algorithms (note the imports), and optimizing various other hyperparameters (especially with LogisticRegression() having so many).  Even when I change the hyperparameters, it doesn't seem to matter too much, and just changing the regularization term doesn't exactly translate to methods like LinearDiscriminantAnalysis (I use eigen solver) and RandomForestClassifier.\n",
    "\n",
    "Any input or help on how to circumvent this issue I am having would be appreciated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13bea55-b98f-419a-9916-a091ac664563",
   "metadata": {},
   "source": [
    "Create and Standardize Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "234bb852-38fe-47d0-8d56-df6be7dd454a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.93813556, -0.85595284,  0.77637696, ..., -0.80338187,\n",
       "         -0.81416873,  0.05104663],\n",
       "        [-2.15195234, -0.85801456,  1.29040257, ..., -1.01328675,\n",
       "          0.29514178, -0.38691768],\n",
       "        [-0.98432038, -1.63938321,  0.3354797 , ..., -0.03998931,\n",
       "          0.63793917, -0.93734998],\n",
       "        ...,\n",
       "        [-0.65248744,  1.20235574, -0.51231733, ..., -0.11017948,\n",
       "         -0.17404411,  1.94768843],\n",
       "        [-2.66028583, -0.96384227, -0.77210554, ..., -1.12739269,\n",
       "         -1.50948637,  0.66149758],\n",
       "        [-1.05319369, -1.48888213, -2.24927062, ...,  1.23250692,\n",
       "         -2.51510765,  0.08806142]]),\n",
       " array([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "        0, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = make_classification(n_samples = 200, n_features = 20, n_informative = 10, flip_y = 0.1, class_sep = 0.5,random_state = 1)\n",
    "x0 = scaler.fit_transform(x)\n",
    "x0,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc9d4c-e4ba-4e7b-be36-24161429b677",
   "metadata": {},
   "source": [
    "Perform Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5587ac8-2267-41f7-a31d-b6a2a3c6bfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'solver': 'saga'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrain, XTest, YTrain, YTest = train_test_split(x0,y,test_size = 0.3, stratify = y, random_state = 8675309)\n",
    "\n",
    "C = [1e-4, 1e-3, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "solver = ['saga','sag']\n",
    "\n",
    "param_grid = {'C':C,\n",
    "              'solver': solver,\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(random_state = 42), param_grid, scoring = 'accuracy', cv = LeaveOneOut())\n",
    "clf.fit(XTrain,YTrain)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9926e58a-d0f8-4673-a197-999ab67df4c1",
   "metadata": {},
   "source": [
    "Create the estimator with the parameters from hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "870cd829-7c81-4e9b-a204-7044ed141213",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(C = 0.1, solver = 'saga') #as per best params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acccbf90-f9c0-49e5-9d66-c31274df12bc",
   "metadata": {},
   "source": [
    "Compare the Cross-Validation Accuracy and the Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b251a86b-4526-4b2a-a3ae-81ca8960e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOCV Accuracy on Training Data ONLY is:  0.5928571428571429\n",
      "Testing Accuracy is:  0.55\n"
     ]
    }
   ],
   "source": [
    "clf = clone(log)\n",
    "model_ = clf.fit(XTrain, YTrain)\n",
    "y_pred = model_.predict(XTest)\n",
    "print('LOOCV Accuracy on Training Data ONLY is: ', np.mean(cross_val_score(log,XTrain, YTrain, scoring = 'accuracy', cv = LeaveOneOut())))\n",
    "print('Testing Accuracy is: ', accuracy_score(YTest, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf28a25f-bcc7-47fa-b8b5-9a174665f0be",
   "metadata": {},
   "source": [
    "Looks normal, right? But if we compare the average accuracy over 1000 train/test splits..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a7e1e535-f53c-42d8-92e7-39313ba33659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation on Training Set Accuracy over 1000 iterations:  0.5822571428571428\n",
      "Testing Accuracy over 1000 iterations (same splits):  0.5929666666666666\n"
     ]
    }
   ],
   "source": [
    "acc = np.zeros((1000,))\n",
    "for i in range(1000):\n",
    "    XTrain, XTest, YTrain, YTest = train_test_split(x0,y,test_size = 0.3, stratify = y, random_state = i**2)\n",
    "    alg = clone(log) #This is to make sure I am not accidentally fitting/predicting with a previously fitted estimator\n",
    "    acc[i] = np.mean(cross_val_score(alg,XTrain, YTrain,scoring = 'accuracy', cv = LeaveOneOut()))\n",
    "    \n",
    "t_acc = np.zeros((1000,))\n",
    "for i in range(1000):\n",
    "    Model = [] #Reset each time\n",
    "    XTrain, XTest, YTrain, YTest = train_test_split(x0,y,test_size = 0.3, stratify = y, random_state = i**2)\n",
    "    alg = clone(log)\n",
    "    Model = alg.fit(XTrain, YTrain)\n",
    "    y_pred = Model.predict(XTest)\n",
    "    t_acc[i] = accuracy_score(YTest, y_pred)\n",
    "print('Cross Validation on Training Set Accuracy over 1000 iterations: ', np.mean(acc))\n",
    "print('Testing Accuracy over 1000 iterations (same splits): ', np.mean(t_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973fb99c-487e-480c-a046-cb34f83f814a",
   "metadata": {},
   "source": [
    "This is a little strange, the t_acc should not be larger than acc... is there any strange behavior looking at the individual data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de7d53c0-26b1-4c64-8db6-7cea62f839c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.57857143, 0.51428571, 0.58571429, 0.61428571, 0.63571429,\n",
       "        0.6       , 0.55714286, 0.57142857, 0.57857143, 0.58571429]),\n",
       " array([0.56666667, 0.65      , 0.6       , 0.63333333, 0.56666667,\n",
       "        0.56666667, 0.65      , 0.66666667, 0.7       , 0.51666667]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc[0:10], t_acc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e5be39-1aac-4cd2-ab56-237faa7b394e",
   "metadata": {},
   "source": [
    "Not really.  What happens if I fit the estimator on the original train test split used to optimize the hyperparams, and then test a bunch of random test sets on this model?  Eventually there should be some test sets that perform much poorer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12ec4cc9-aa9d-4ea5-83b0-1ee3f3a5af6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy on Original Split used in Hyperparam. Optimization:  0.5928571428571429\n",
      "Test Accuracy with 1000 Test Sets all on the same model:  0.6561500000000001\n"
     ]
    }
   ],
   "source": [
    "clf = clone(log)\n",
    "XTrain, XTest, YTrain, YTest = train_test_split(x0,y,test_size = 0.3, stratify = y, random_state = 8675309)\n",
    "print('Cross Validation Accuracy on Original Split used in Hyperparam. Optimization: ',np.mean(cross_val_score(clf,XTrain, YTrain, scoring = 'accuracy', cv = LeaveOneOut()))) #verify previous result\n",
    "\n",
    "\n",
    "Model_ = clf.fit(XTrain, YTrain) #This remains fixed, outside the loop\n",
    "\n",
    "t_acc2 = np.zeros((1000,))\n",
    "for i in range(1000):\n",
    "    XTrain, XTest, YTrain, YTest = train_test_split(x0,y,test_size = 0.3, stratify = y, random_state = i**2)\n",
    "    y_pred = Model_.predict(XTest)\n",
    "    t_acc2[i] = accuracy_score(YTest, y_pred)\n",
    "\n",
    "print('Test Accuracy with 1000 Test Sets all on the same model: ', np.mean(t_acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b32787-5416-42e5-aadc-1ebde5acb014",
   "metadata": {},
   "source": [
    "Alright.  This behavior could possibly be attributed to the fact that the accuracy across the board is low, because its not like the accuracy should be lower than 0.5.  What happens when we import synthetic data that can be much more easily classified?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf581ef-957e-4899-814f-4bc3be9552ed",
   "metadata": {},
   "source": [
    "Below is a synthetic dataset (can't be super explicit about the generator, but it shouldn't matter too much for this issue) with 2000 features and 200 samples.  The data was scaled using StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a84d264-7c40-45b7-b4ec-5272e0d1402c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.727939</td>\n",
       "      <td>-0.408282</td>\n",
       "      <td>-0.702795</td>\n",
       "      <td>-0.709684</td>\n",
       "      <td>-0.548450</td>\n",
       "      <td>0.428379</td>\n",
       "      <td>1.498732</td>\n",
       "      <td>-0.775337</td>\n",
       "      <td>3.545692</td>\n",
       "      <td>-0.727709</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.991115</td>\n",
       "      <td>-0.332513</td>\n",
       "      <td>0.080433</td>\n",
       "      <td>-0.964743</td>\n",
       "      <td>1.748908</td>\n",
       "      <td>-0.827986</td>\n",
       "      <td>0.301926</td>\n",
       "      <td>-0.688749</td>\n",
       "      <td>-0.668324</td>\n",
       "      <td>0.506503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.948129</td>\n",
       "      <td>-1.534414</td>\n",
       "      <td>-0.317488</td>\n",
       "      <td>-0.289633</td>\n",
       "      <td>-0.626682</td>\n",
       "      <td>1.084706</td>\n",
       "      <td>-0.821099</td>\n",
       "      <td>-0.744635</td>\n",
       "      <td>2.250363</td>\n",
       "      <td>-0.203531</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.097877</td>\n",
       "      <td>0.322854</td>\n",
       "      <td>-0.238746</td>\n",
       "      <td>-0.220579</td>\n",
       "      <td>0.723101</td>\n",
       "      <td>-0.654636</td>\n",
       "      <td>1.379500</td>\n",
       "      <td>-0.322806</td>\n",
       "      <td>-1.212171</td>\n",
       "      <td>-0.301734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.570152</td>\n",
       "      <td>0.527955</td>\n",
       "      <td>-1.158218</td>\n",
       "      <td>-0.198142</td>\n",
       "      <td>0.683628</td>\n",
       "      <td>0.860689</td>\n",
       "      <td>0.635202</td>\n",
       "      <td>0.239283</td>\n",
       "      <td>1.327666</td>\n",
       "      <td>-0.108473</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.584675</td>\n",
       "      <td>-0.466286</td>\n",
       "      <td>2.254270</td>\n",
       "      <td>-0.163865</td>\n",
       "      <td>1.918743</td>\n",
       "      <td>0.823504</td>\n",
       "      <td>0.669029</td>\n",
       "      <td>-1.035258</td>\n",
       "      <td>0.283971</td>\n",
       "      <td>2.313788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.889491</td>\n",
       "      <td>-0.467942</td>\n",
       "      <td>-0.665719</td>\n",
       "      <td>-0.870582</td>\n",
       "      <td>-0.212309</td>\n",
       "      <td>2.909781</td>\n",
       "      <td>0.980574</td>\n",
       "      <td>-1.173218</td>\n",
       "      <td>3.023919</td>\n",
       "      <td>-0.979268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185068</td>\n",
       "      <td>-0.264236</td>\n",
       "      <td>-0.235187</td>\n",
       "      <td>-0.305832</td>\n",
       "      <td>0.405041</td>\n",
       "      <td>-0.340313</td>\n",
       "      <td>2.758644</td>\n",
       "      <td>-0.896897</td>\n",
       "      <td>-0.917175</td>\n",
       "      <td>-0.152348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.781059</td>\n",
       "      <td>1.080958</td>\n",
       "      <td>-1.088813</td>\n",
       "      <td>-0.584897</td>\n",
       "      <td>1.375659</td>\n",
       "      <td>-0.338295</td>\n",
       "      <td>0.665265</td>\n",
       "      <td>0.539406</td>\n",
       "      <td>-0.517239</td>\n",
       "      <td>-0.438098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281022</td>\n",
       "      <td>-0.573114</td>\n",
       "      <td>1.225223</td>\n",
       "      <td>-1.028783</td>\n",
       "      <td>-0.215001</td>\n",
       "      <td>1.411628</td>\n",
       "      <td>-0.073375</td>\n",
       "      <td>-1.208114</td>\n",
       "      <td>-0.205165</td>\n",
       "      <td>1.130371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1.727939 -0.408282 -0.702795 -0.709684 -0.548450  0.428379  1.498732   \n",
       "1 -0.948129 -1.534414 -0.317488 -0.289633 -0.626682  1.084706 -0.821099   \n",
       "2  0.570152  0.527955 -1.158218 -0.198142  0.683628  0.860689  0.635202   \n",
       "3  0.889491 -0.467942 -0.665719 -0.870582 -0.212309  2.909781  0.980574   \n",
       "4  0.781059  1.080958 -1.088813 -0.584897  1.375659 -0.338295  0.665265   \n",
       "\n",
       "          7         8         9  ...      1990      1991      1992      1993  \\\n",
       "0 -0.775337  3.545692 -0.727709  ... -1.991115 -0.332513  0.080433 -0.964743   \n",
       "1 -0.744635  2.250363 -0.203531  ... -1.097877  0.322854 -0.238746 -0.220579   \n",
       "2  0.239283  1.327666 -0.108473  ... -0.584675 -0.466286  2.254270 -0.163865   \n",
       "3 -1.173218  3.023919 -0.979268  ...  0.185068 -0.264236 -0.235187 -0.305832   \n",
       "4  0.539406 -0.517239 -0.438098  ...  0.281022 -0.573114  1.225223 -1.028783   \n",
       "\n",
       "       1994      1995      1996      1997      1998      1999  \n",
       "0  1.748908 -0.827986  0.301926 -0.688749 -0.668324  0.506503  \n",
       "1  0.723101 -0.654636  1.379500 -0.322806 -1.212171 -0.301734  \n",
       "2  1.918743  0.823504  0.669029 -1.035258  0.283971  2.313788  \n",
       "3  0.405041 -0.340313  2.758644 -0.896897 -0.917175 -0.152348  \n",
       "4 -0.215001  1.411628 -0.073375 -1.208114 -0.205165  1.130371  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('Total_Dataset.csv')\n",
    "X = X.drop(columns = 'Unnamed: 0')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99a3a52-69ee-4d9a-8ecb-3d2b91e07389",
   "metadata": {},
   "source": [
    "The Class Membership is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7ea149c0-6b13-4b6a-8b70-afb8dcb1642d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.zeros((200,))\n",
    "Y[0:100] = 1\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341a100-b298-4fe9-a9f8-b6a4830fb614",
   "metadata": {},
   "source": [
    "After some feature selection (again, don't want to be super explicit but it shouldn't matter), I get 13 features left (200 x 13 dataset).\n",
    "\n",
    "A disclaimer: The below data is a shuffled version of the above data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ef8030ba-ecb9-49ce-a1b7-8ab6b06429ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.124679</td>\n",
       "      <td>-0.511666</td>\n",
       "      <td>0.727900</td>\n",
       "      <td>0.240307</td>\n",
       "      <td>0.373149</td>\n",
       "      <td>-0.567941</td>\n",
       "      <td>-0.830887</td>\n",
       "      <td>-0.003841</td>\n",
       "      <td>0.070714</td>\n",
       "      <td>-0.815380</td>\n",
       "      <td>0.115287</td>\n",
       "      <td>-0.662071</td>\n",
       "      <td>0.670274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.925660</td>\n",
       "      <td>-0.243895</td>\n",
       "      <td>-1.397182</td>\n",
       "      <td>0.741588</td>\n",
       "      <td>0.515703</td>\n",
       "      <td>-0.904077</td>\n",
       "      <td>0.768483</td>\n",
       "      <td>-0.799157</td>\n",
       "      <td>0.808489</td>\n",
       "      <td>0.901693</td>\n",
       "      <td>-0.945580</td>\n",
       "      <td>-0.203934</td>\n",
       "      <td>-1.323254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.277331</td>\n",
       "      <td>-0.347020</td>\n",
       "      <td>0.773039</td>\n",
       "      <td>0.635408</td>\n",
       "      <td>0.357638</td>\n",
       "      <td>0.055553</td>\n",
       "      <td>-0.706853</td>\n",
       "      <td>-0.115096</td>\n",
       "      <td>0.609852</td>\n",
       "      <td>-0.402179</td>\n",
       "      <td>-0.203096</td>\n",
       "      <td>-0.627237</td>\n",
       "      <td>0.554816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.355087</td>\n",
       "      <td>1.832806</td>\n",
       "      <td>-0.502055</td>\n",
       "      <td>0.519937</td>\n",
       "      <td>0.496318</td>\n",
       "      <td>1.309723</td>\n",
       "      <td>1.648091</td>\n",
       "      <td>1.416451</td>\n",
       "      <td>0.281562</td>\n",
       "      <td>1.501264</td>\n",
       "      <td>1.372674</td>\n",
       "      <td>1.911366</td>\n",
       "      <td>-0.475303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.656498</td>\n",
       "      <td>-0.147177</td>\n",
       "      <td>-0.347383</td>\n",
       "      <td>-0.792749</td>\n",
       "      <td>-1.088772</td>\n",
       "      <td>1.005990</td>\n",
       "      <td>-0.648401</td>\n",
       "      <td>-0.714435</td>\n",
       "      <td>-0.720153</td>\n",
       "      <td>-0.625142</td>\n",
       "      <td>-0.779137</td>\n",
       "      <td>0.053101</td>\n",
       "      <td>-0.425556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.124679 -0.511666  0.727900  0.240307  0.373149 -0.567941 -0.830887   \n",
       "1 -0.925660 -0.243895 -1.397182  0.741588  0.515703 -0.904077  0.768483   \n",
       "2 -0.277331 -0.347020  0.773039  0.635408  0.357638  0.055553 -0.706853   \n",
       "3  1.355087  1.832806 -0.502055  0.519937  0.496318  1.309723  1.648091   \n",
       "4 -0.656498 -0.147177 -0.347383 -0.792749 -1.088772  1.005990 -0.648401   \n",
       "\n",
       "          7         8         9        10        11        12  \n",
       "0 -0.003841  0.070714 -0.815380  0.115287 -0.662071  0.670274  \n",
       "1 -0.799157  0.808489  0.901693 -0.945580 -0.203934 -1.323254  \n",
       "2 -0.115096  0.609852 -0.402179 -0.203096 -0.627237  0.554816  \n",
       "3  1.416451  0.281562  1.501264  1.372674  1.911366 -0.475303  \n",
       "4 -0.714435 -0.720153 -0.625142 -0.779137  0.053101 -0.425556  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0 = pd.read_csv('simData.csv')\n",
    "X0 = X0.drop(columns = 'Unnamed: 0')\n",
    "X0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a5b0afdd-d6ad-487b-bd86-c9c9c0456801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0  1.0\n",
       "1  1.0\n",
       "2  1.0\n",
       "3  0.0\n",
       "4  0.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y0 = pd.read_csv('simY.csv')\n",
    "Y0 = Y0.drop(columns = 'Unnamed: 0')\n",
    "Y0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb88f06-f6bb-49b5-a961-c7a1ddd5e531",
   "metadata": {},
   "source": [
    "I won't be demonstrating on the 200x2000 dataset for the sake of verbosity, since I primarily care about the data post-feature selection anyways.  If you would like to try it, it should be fairly plug-and-play friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c68efd1a-4565-42ae-b19b-a242b451c9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'solver': 'saga'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrain, XTest, YTrain, YTest = train_test_split(X0,Y0,test_size = 0.3, stratify = Y0, random_state = 42)\n",
    "\n",
    "C = [1e-4, 1e-3, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "solver = ['saga','sag']\n",
    "\n",
    "param_grid = {'C':C,\n",
    "              'solver': solver,\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(random_state = 42), param_grid, scoring = 'accuracy', cv = LeaveOneOut())\n",
    "clf.fit(XTrain,YTrain)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "30d6fbf4-15fa-4d24-8b2f-24491164d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C = 0.01, solver = 'saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e6e9b257-c220-4bdc-857b-7f2fced99bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOCV Accuracy on Training Data ONLY is:  0.9571428571428572\n",
      "Testing Accuracy is:  0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "clf = clone(log)\n",
    "model_ = clf.fit(XTrain, YTrain)\n",
    "y_pred = model_.predict(XTest)\n",
    "print('LOOCV Accuracy on Training Data ONLY is: ', np.mean(cross_val_score(lr,XTrain, YTrain, scoring = 'accuracy', cv = LeaveOneOut())))\n",
    "print('Testing Accuracy is: ', accuracy_score(YTest, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6351b1-28a2-4241-a776-26c09e1325d2",
   "metadata": {},
   "source": [
    "This already doesn't bode well. What happens when we average it over 1000 iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d6f6f066-1506-48f7-8200-0fd739fc58a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation on Training Set Accuracy over 1000 iterations:  0.9694071428571427\n",
      "Testing Accuracy over 1000 iterations (same splits):  0.9705333333333331\n"
     ]
    }
   ],
   "source": [
    "sim_acc = np.zeros((1000,))\n",
    "for i in range(1000):\n",
    "    XTrain, XTest, YTrain, YTest = train_test_split(X0,Y0,test_size = 0.3, stratify = Y0, random_state = i**2)\n",
    "    alg = clone(log) #This is to make sure I am not accidentally fitting/predicting with a previously fitted estimator\n",
    "    sim_acc[i] = np.mean(cross_val_score(alg,XTrain, YTrain,scoring = 'accuracy', cv = LeaveOneOut()))\n",
    "    \n",
    "sim_t_acc = np.zeros((1000,))\n",
    "for i in range(1000):\n",
    "    Model = [] #Reset each time\n",
    "    XTrain, XTest, YTrain, YTest = train_test_split(X0,Y0,test_size = 0.3, stratify = Y0, random_state = i**2)\n",
    "    alg = clone(log)\n",
    "    Model = alg.fit(XTrain, YTrain)\n",
    "    y_pred = Model.predict(XTest)\n",
    "    sim_t_acc[i] = accuracy_score(YTest, y_pred)\n",
    "print('Cross Validation on Training Set Accuracy over 1000 iterations: ', np.mean(sim_acc))\n",
    "print('Testing Accuracy over 1000 iterations (same splits): ', np.mean(sim_t_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "22a56550-4626-455c-bae3-a744ca4cf6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy on Original Split used in Hyperparam. Optimization:  0.9785714285714285\n",
      "Test Accuracy with 1000 Test Sets all on the same model:  0.9805999999999999\n"
     ]
    }
   ],
   "source": [
    "clf2 = clone(log)\n",
    "print('Cross Validation Accuracy on Original Split used in Hyperparam. Optimization: ',np.mean(cross_val_score(clf,XTrain, YTrain, scoring = 'accuracy', cv = LeaveOneOut()))) #verify previous result\n",
    "\n",
    "XTrain, XTest, YTrain, YTest = train_test_split(X0,Y0,test_size = 0.3, stratify = Y0, random_state = 8675309)\n",
    "Model_ = clf2.fit(XTrain, YTrain) #This remains fixed, outside the loop\n",
    "\n",
    "sim_t_acc2 = np.zeros((1000,))\n",
    "for i in range(1000):\n",
    "    XTrain, XTest, YTrain, YTest = train_test_split(X0,Y0,test_size = 0.3, stratify = Y0, random_state = i**2)\n",
    "    y_pred = Model_.predict(XTest)\n",
    "    sim_t_acc2[i] = accuracy_score(YTest, y_pred)\n",
    "\n",
    "print('Test Accuracy with 1000 Test Sets all on the same model: ', np.mean(sim_t_acc2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
